/**
 * LR parser generated by the Syntax tool.
 *
 * https://www.npmjs.com/package/syntax-cli
 *
 *   npm install -g syntax-cli
 *
 *   syntax-cli --help
 *
 * To regenerate run:
 *
 *   syntax-cli \
 *     --grammar ~/path-to-grammar-file \
 *     --mode <parsing-mode> \
 *     --output ~/path-to-output-parser-file.js
 */

'use strict'

/**
 * Matched token text.
 */
let yytext

/**
 * Length of the matched token text.
 */
let yyleng

/**
 * Storage object.
 */
let yy = {}

/**
 * Result of semantic action.
 */
let __

/**
 * Result location object.
 */
let __loc

function yyloc(start, end) {
	if (!yy.options.captureLocations) {
		return null
	}

	// Epsilon doesn't produce location.
	if (!start || !end) {
		return start || end
	}

	return {
		startOffset: start.startOffset,
		endOffset: end.endOffset,
		startLine: start.startLine,
		endLine: end.endLine,
		startColumn: start.startColumn,
		endColumn: end.endColumn,
	}
}

const EOF = '$'

/**
 * List of productions (generated by Syntax tool).
 */
const productions = [
	[
		-1,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		0,
		1,
		(_1) => {
			__ = {
				type: 'Program',
				body: _1,
			}
		},
	],
	[
		1,
		2,
		(_1, _2) => {
			__ = [..._1, _2]
		},
	],
	[
		1,
		1,
		(_1) => {
			__ = [_1]
		},
	],
	[
		2,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		2,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		2,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		3,
		8,
		(_1, _2, _3, _4, _5, _6, _7, _8) => {
			__ = {
				type: 'FunctionDeclaration',
				id: _2,
				params: [],
				returnType: _4,
				body: {
					type: 'BlockStatement',
					body: _7,
				},
			}
		},
	],
	[
		3,
		9,
		(_1, _2, _3, _4, _5, _6, _7, _8, _9) => {
			__ = {
				type: 'FunctionDeclaration',
				id: _2,
				params: _3,
				returnType: _5,
				body: {
					type: 'BlockStatement',
					body: _8,
				},
			}
		},
	],
	[
		4,
		2,
		(_1, _2) => {
			__ = []
		},
	],
	[
		4,
		3,
		(_1, _2, _3) => {
			__ = _2
		},
	],
	[
		5,
		3,
		(_1, _2, _3) => {
			__ = [..._1, _3]
		},
	],
	[
		5,
		1,
		(_1) => {
			__ = [_1]
		},
	],
	[
		6,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		6,
		3,
		(_1, _2, _3) => {
			__ = { type: 'Identifier', name: _1, typing: _3 }
		},
	],
	[
		7,
		3,
		(_1, _2, _3) => {
			__ = [..._1, _3]
		},
	],
	[
		7,
		1,
		(_1) => {
			__ = [_1]
		},
	],
	[
		7,
		0,
		() => {
			__ = []
		},
	],
	[
		8,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		8,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		9,
		2,
		(_1, _2) => {
			__ = [..._1, _2]
		},
	],
	[
		9,
		1,
		(_1) => {
			__ = [_1]
		},
	],
	[
		10,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		10,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		10,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		10,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		10,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		11,
		3,
		(_1, _2, _3) => {
			__ = {
				type: 'ImportDeclaration',
				source: _2,
			}
		},
	],
	[
		12,
		5,
		(_1, _2, _3, _4, _5) => {
			__ = {
				type: 'VariableDeclaration',
				mutable: true,
				id: _1,
				typing: _3,
				value: _5,
			}
		},
	],
	[
		12,
		6,
		(_1, _2, _3, _4, _5, _6) => {
			__ = {
				type: 'VariableDeclaration',
				mutable: false,
				id: _2,
				typing: _4,
				value: _6,
			}
		},
	],
	[
		13,
		3,
		(_1, _2, _3) => {
			__ = {
				type: 'ReturnStatement',
				argument: _2,
			}
		},
	],
	[
		14,
		3,
		(_1, _2, _3) => {
			__ = {
				type: 'Literal',
				std: true,
				value: _2,
			}
		},
	],
	[
		14,
		1,
		(_1) => {
			__ = {
				..._1,
				std: false,
			}
		},
	],
	[
		15,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		15,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		15,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		16,
		4,
		(_1, _2, _3, _4) => {
			__ = {
				type: 'CallExpression',
				callee: _1,
				arguments: _3,
			}
		},
	],
	[
		17,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		18,
		3,
		(_1, _2, _3) => {
			__ = {
				type: 'BinaryExpression',
				op: _2,
				left: _1,
				right: _3,
			}
		},
	],
	[
		19,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		19,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		20,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		20,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		21,
		1,
		(_1) => {
			__ = {
				type: 'Identifier',
				name: _1,
			}
		},
	],
	[
		22,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		22,
		1,
		(_1) => {
			__ = _1
		},
	],
	[
		23,
		1,
		(_1) => {
			__ = {
				type: 'Literal',
				typing: 'int',
				value: Number(_1),
			}
		},
	],
	[
		24,
		1,
		(_1) => {
			__ = {
				type: 'Literal',
				typing: '*char',
				value: _1,
			}
		},
	],
	[
		25,
		1,
		(_1) => {
			__ = null
		},
	],
]

/**
 * Encoded tokens map.
 */
const tokens = {
	KW_FN: '26',
	TOK_ARROW: '27',
	KW_DO: '28',
	EOL: '29',
	KW_END: '30',
	TOK_LPAREN: '31',
	TOK_RPAREN: '32',
	TOK_COMMA: '33',
	TOK_COLON: '34',
	ImportStatement: '35',
	KW_IMPORT: '36',
	TOK_EQUAL: '37',
	KW_CONST: '38',
	KW_RETURN: '39',
	TOK_LT: '40',
	IMPORT_ID: '41',
	TOK_GT: '42',
	TOK_PLUS: '43',
	TOK_MINUS: '44',
	IDENTIFIER: '45',
	INTEGER: '46',
	STRING: '47',
	$: '48',
}

/**
 * Parsing table (generated by Syntax tool).
 */
const table = [
	{
		0: 1,
		1: 2,
		2: 6,
		3: 10,
		10: 3,
		11: 11,
		12: 12,
		13: 5,
		15: 7,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 15,
		22: 20,
		23: 23,
		24: 24,
		25: 8,
		26: 's13',
		29: 's27',
		35: 's4',
		36: 's14',
		38: 's16',
		39: 's9',
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{ 48: 'acc' },
	{
		2: 6,
		3: 10,
		10: 28,
		11: 11,
		12: 12,
		13: 5,
		15: 7,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 15,
		22: 20,
		23: 23,
		24: 24,
		25: 8,
		26: 's13',
		29: 's27',
		35: 's4',
		36: 's14',
		38: 's16',
		39: 's9',
		45: 's17',
		46: 's25',
		47: 's26',
		48: 'r1',
	},
	{
		26: 'r3',
		29: 'r3',
		35: 'r3',
		36: 'r3',
		38: 'r3',
		39: 'r3',
		45: 'r3',
		46: 'r3',
		47: 'r3',
		48: 'r3',
	},
	{
		26: 'r22',
		29: 'r22',
		30: 'r22',
		35: 'r22',
		36: 'r22',
		38: 'r22',
		39: 'r22',
		45: 'r22',
		46: 'r22',
		47: 'r22',
		48: 'r22',
	},
	{
		26: 'r23',
		29: 'r23',
		30: 'r23',
		35: 'r23',
		36: 'r23',
		38: 'r23',
		39: 'r23',
		45: 'r23',
		46: 'r23',
		47: 'r23',
		48: 'r23',
	},
	{
		26: 'r24',
		29: 'r24',
		30: 'r24',
		35: 'r24',
		36: 'r24',
		38: 'r24',
		39: 'r24',
		45: 'r24',
		46: 'r24',
		47: 'r24',
		48: 'r24',
	},
	{
		26: 'r25',
		29: 'r25',
		30: 'r25',
		35: 'r25',
		36: 'r25',
		38: 'r25',
		39: 'r25',
		45: 'r25',
		46: 'r25',
		47: 'r25',
		48: 'r25',
	},
	{
		26: 'r26',
		29: 'r26',
		30: 'r26',
		35: 'r26',
		36: 'r26',
		38: 'r26',
		39: 'r26',
		45: 'r26',
		46: 'r26',
		47: 'r26',
		48: 'r26',
	},
	{
		15: 29,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 30,
		22: 20,
		23: 23,
		24: 24,
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{
		26: 'r4',
		29: 'r4',
		30: 'r4',
		35: 'r4',
		36: 'r4',
		38: 'r4',
		39: 'r4',
		45: 'r4',
		46: 'r4',
		47: 'r4',
		48: 'r4',
	},
	{
		26: 'r5',
		29: 'r5',
		30: 'r5',
		35: 'r5',
		36: 'r5',
		38: 'r5',
		39: 'r5',
		45: 'r5',
		46: 'r5',
		47: 'r5',
		48: 'r5',
	},
	{
		26: 'r6',
		29: 'r6',
		30: 'r6',
		35: 'r6',
		36: 'r6',
		38: 'r6',
		39: 'r6',
		45: 'r6',
		46: 'r6',
		47: 'r6',
		48: 'r6',
	},
	{ 21: 46, 45: 's17' },
	{ 14: 57, 24: 59, 40: 's58', 47: 's26' },
	{ 31: 's32', 34: 's63', 43: 'r41', 44: 'r41' },
	{ 21: 67, 45: 's17' },
	{
		26: 'r43',
		27: 'r43',
		28: 'r43',
		29: 'r43',
		30: 'r43',
		31: 'r43',
		32: 'r43',
		33: 'r43',
		34: 'r43',
		35: 'r43',
		36: 'r43',
		37: 'r43',
		38: 'r43',
		39: 'r43',
		43: 'r43',
		44: 'r43',
		45: 'r43',
		46: 'r43',
		47: 'r43',
		48: 'r43',
	},
	{
		26: 'r33',
		29: 'r33',
		30: 'r33',
		35: 'r33',
		36: 'r33',
		38: 'r33',
		39: 'r33',
		45: 'r33',
		46: 'r33',
		47: 'r33',
		48: 'r33',
	},
	{
		26: 'r34',
		29: 'r34',
		30: 'r34',
		35: 'r34',
		36: 'r34',
		38: 'r34',
		39: 'r34',
		45: 'r34',
		46: 'r34',
		47: 'r34',
		48: 'r34',
	},
	{
		26: 'r35',
		29: 'r35',
		30: 'r35',
		35: 'r35',
		36: 'r35',
		38: 'r35',
		39: 'r35',
		43: 'r42',
		44: 'r42',
		45: 'r35',
		46: 'r35',
		47: 'r35',
		48: 'r35',
	},
	{
		26: 'r37',
		29: 'r37',
		30: 'r37',
		35: 'r37',
		36: 'r37',
		38: 'r37',
		39: 'r37',
		45: 'r37',
		46: 'r37',
		47: 'r37',
		48: 'r37',
	},
	{ 19: 40, 43: 's41', 44: 's42' },
	{
		26: 'r44',
		29: 'r44',
		30: 'r44',
		32: 'r44',
		33: 'r44',
		35: 'r44',
		36: 'r44',
		38: 'r44',
		39: 'r44',
		43: 'r44',
		44: 'r44',
		45: 'r44',
		46: 'r44',
		47: 'r44',
		48: 'r44',
	},
	{
		26: 'r45',
		29: 'r45',
		30: 'r45',
		32: 'r45',
		33: 'r45',
		35: 'r45',
		36: 'r45',
		38: 'r45',
		39: 'r45',
		43: 'r45',
		44: 'r45',
		45: 'r45',
		46: 'r45',
		47: 'r45',
		48: 'r45',
	},
	{
		26: 'r46',
		29: 'r46',
		30: 'r46',
		32: 'r46',
		33: 'r46',
		35: 'r46',
		36: 'r46',
		38: 'r46',
		39: 'r46',
		43: 'r46',
		44: 'r46',
		45: 'r46',
		46: 'r46',
		47: 'r46',
		48: 'r46',
	},
	{
		26: 'r47',
		29: 'r47',
		30: 'r47',
		32: 'r47',
		33: 'r47',
		35: 'r47',
		36: 'r47',
		38: 'r47',
		39: 'r47',
		43: 'r47',
		44: 'r47',
		45: 'r47',
		46: 'r47',
		47: 'r47',
		48: 'r47',
	},
	{
		26: 'r48',
		29: 'r48',
		30: 'r48',
		35: 'r48',
		36: 'r48',
		38: 'r48',
		39: 'r48',
		45: 'r48',
		46: 'r48',
		47: 'r48',
		48: 'r48',
	},
	{
		26: 'r2',
		29: 'r2',
		35: 'r2',
		36: 'r2',
		38: 'r2',
		39: 'r2',
		45: 'r2',
		46: 'r2',
		47: 'r2',
		48: 'r2',
	},
	{ 29: 's31' },
	{ 31: 's32', 43: 'r41', 44: 'r41' },
	{
		26: 'r30',
		29: 'r30',
		30: 'r30',
		35: 'r30',
		36: 'r30',
		38: 'r30',
		39: 'r30',
		45: 'r30',
		46: 'r30',
		47: 'r30',
		48: 'r30',
	},
	{
		7: 33,
		8: 34,
		21: 36,
		22: 35,
		23: 23,
		24: 24,
		32: 'r17',
		33: 'r17',
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{ 32: 's37', 33: 's38' },
	{ 32: 'r16', 33: 'r16' },
	{ 32: 'r18', 33: 'r18' },
	{ 32: 'r19', 33: 'r19' },
	{
		26: 'r36',
		29: 'r36',
		30: 'r36',
		35: 'r36',
		36: 'r36',
		38: 'r36',
		39: 'r36',
		45: 'r36',
		46: 'r36',
		47: 'r36',
		48: 'r36',
	},
	{ 8: 39, 21: 36, 22: 35, 23: 23, 24: 24, 45: 's17', 46: 's25', 47: 's26' },
	{ 32: 'r15', 33: 'r15' },
	{ 20: 43, 21: 44, 22: 45, 23: 23, 24: 24, 45: 's17', 46: 's25', 47: 's26' },
	{ 45: 'r39', 46: 'r39', 47: 'r39' },
	{ 45: 'r40', 46: 'r40', 47: 'r40' },
	{
		26: 'r38',
		29: 'r38',
		30: 'r38',
		35: 'r38',
		36: 'r38',
		38: 'r38',
		39: 'r38',
		45: 'r38',
		46: 'r38',
		47: 'r38',
		48: 'r38',
	},
	{
		26: 'r41',
		29: 'r41',
		30: 'r41',
		35: 'r41',
		36: 'r41',
		38: 'r41',
		39: 'r41',
		45: 'r41',
		46: 'r41',
		47: 'r41',
		48: 'r41',
	},
	{
		26: 'r42',
		29: 'r42',
		30: 'r42',
		35: 'r42',
		36: 'r42',
		38: 'r42',
		39: 'r42',
		45: 'r42',
		46: 'r42',
		47: 'r42',
		48: 'r42',
	},
	{ 4: 48, 27: 's47', 31: 's49' },
	{ 21: 50, 45: 's17' },
	{ 27: 's72' },
	{ 5: 79, 6: 80, 21: 81, 32: 's78', 45: 's17' },
	{ 28: 's51' },
	{ 29: 's52' },
	{
		2: 6,
		3: 10,
		9: 53,
		10: 54,
		11: 11,
		12: 12,
		13: 5,
		15: 7,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 15,
		22: 20,
		23: 23,
		24: 24,
		25: 8,
		26: 's13',
		29: 's27',
		35: 's4',
		36: 's14',
		38: 's16',
		39: 's9',
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{
		2: 6,
		3: 10,
		10: 56,
		11: 11,
		12: 12,
		13: 5,
		15: 7,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 15,
		22: 20,
		23: 23,
		24: 24,
		25: 8,
		26: 's13',
		29: 's27',
		30: 's55',
		35: 's4',
		36: 's14',
		38: 's16',
		39: 's9',
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{
		26: 'r21',
		29: 'r21',
		30: 'r21',
		35: 'r21',
		36: 'r21',
		38: 'r21',
		39: 'r21',
		45: 'r21',
		46: 'r21',
		47: 'r21',
	},
	{
		26: 'r7',
		29: 'r7',
		30: 'r7',
		35: 'r7',
		36: 'r7',
		38: 'r7',
		39: 'r7',
		45: 'r7',
		46: 'r7',
		47: 'r7',
		48: 'r7',
	},
	{
		26: 'r20',
		29: 'r20',
		30: 'r20',
		35: 'r20',
		36: 'r20',
		38: 'r20',
		39: 'r20',
		45: 'r20',
		46: 'r20',
		47: 'r20',
	},
	{ 29: 's60' },
	{ 41: 's61' },
	{ 29: 'r32' },
	{
		26: 'r27',
		29: 'r27',
		30: 'r27',
		35: 'r27',
		36: 'r27',
		38: 'r27',
		39: 'r27',
		45: 'r27',
		46: 'r27',
		47: 'r27',
		48: 'r27',
	},
	{ 42: 's62' },
	{ 29: 'r31' },
	{ 21: 64, 45: 's17' },
	{ 37: 's65' },
	{
		15: 66,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 30,
		22: 20,
		23: 23,
		24: 24,
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{
		26: 'r28',
		29: 'r28',
		30: 'r28',
		35: 'r28',
		36: 'r28',
		38: 'r28',
		39: 'r28',
		45: 'r28',
		46: 'r28',
		47: 'r28',
		48: 'r28',
	},
	{ 34: 's68' },
	{ 21: 69, 45: 's17' },
	{ 37: 's70' },
	{
		15: 71,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 30,
		22: 20,
		23: 23,
		24: 24,
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{
		26: 'r29',
		29: 'r29',
		30: 'r29',
		35: 'r29',
		36: 'r29',
		38: 'r29',
		39: 'r29',
		45: 'r29',
		46: 'r29',
		47: 'r29',
		48: 'r29',
	},
	{ 21: 73, 45: 's17' },
	{ 28: 's74' },
	{ 29: 's75' },
	{
		2: 6,
		3: 10,
		9: 76,
		10: 54,
		11: 11,
		12: 12,
		13: 5,
		15: 7,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 15,
		22: 20,
		23: 23,
		24: 24,
		25: 8,
		26: 's13',
		29: 's27',
		35: 's4',
		36: 's14',
		38: 's16',
		39: 's9',
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{
		2: 6,
		3: 10,
		10: 56,
		11: 11,
		12: 12,
		13: 5,
		15: 7,
		16: 18,
		17: 19,
		18: 21,
		20: 22,
		21: 15,
		22: 20,
		23: 23,
		24: 24,
		25: 8,
		26: 's13',
		29: 's27',
		30: 's77',
		35: 's4',
		36: 's14',
		38: 's16',
		39: 's9',
		45: 's17',
		46: 's25',
		47: 's26',
	},
	{
		26: 'r8',
		29: 'r8',
		30: 'r8',
		35: 'r8',
		36: 'r8',
		38: 'r8',
		39: 'r8',
		45: 'r8',
		46: 'r8',
		47: 'r8',
		48: 'r8',
	},
	{ 27: 'r9' },
	{ 32: 's82', 33: 's83' },
	{ 32: 'r12', 33: 'r12' },
	{ 32: 'r13', 33: 'r13', 34: 's85' },
	{ 27: 'r10' },
	{ 6: 84, 21: 81, 45: 's17' },
	{ 32: 'r11', 33: 'r11' },
	{ 21: 86, 45: 's17' },
	{ 32: 'r14', 33: 'r14' },
]

/**
 * Parsing stack.
 */
const stack = []

/**
 * Tokenizer instance.
 */
let tokenizer
/**
 * Generic tokenizer used by the parser in the Syntax tool.
 *
 * https://www.npmjs.com/package/syntax-cli
 *
 * See `--custom-tokinzer` to skip this generation, and use a custom one.
 */

const lexRules = [
	[
		/^fn/,
		function () {
			return 'KW_FN'
		},
	],
	[
		/^do/,
		function () {
			return 'KW_DO'
		},
	],
	[
		/^end/,
		function () {
			return 'KW_END'
		},
	],
	[
		/^true/,
		function () {
			return 'KW_TRUE'
		},
	],
	[
		/^false/,
		function () {
			return 'KW_FALSE'
		},
	],
	[
		/^import/,
		function () {
			return 'KW_IMPORT'
		},
	],
	[
		/^object/,
		function () {
			return 'KW_OBJECT'
		},
	],
	[
		/^return/,
		function () {
			return 'KW_RETURN'
		},
	],
	[
		/^const/,
		function () {
			return 'KW_CONST'
		},
	],
	[
		/^\(/,
		function () {
			return 'TOK_LPAREN'
		},
	],
	[
		/^\)/,
		function () {
			return 'TOK_RPAREN'
		},
	],
	[
		/^->/,
		function () {
			return 'TOK_ARROW'
		},
	],
	[
		/^</,
		function () {
			return 'TOK_LT'
		},
	],
	[
		/^>/,
		function () {
			return 'TOK_GT'
		},
	],
	[
		/^\+/,
		function () {
			return 'TOK_PLUS'
		},
	],
	[
		/^-/,
		function () {
			return 'TOK_MINUS'
		},
	],
	[
		/^=/,
		function () {
			return 'TOK_EQUAL'
		},
	],
	[
		/^[\*]$/,
		function () {
			return 'TOK_STAR'
		},
	],
	[
		/^\//,
		function () {
			return 'TOK_SLASH'
		},
	],
	[
		/^:/,
		function () {
			return 'TOK_COLON'
		},
	],
	[
		/^;/,
		function () {
			return 'TOK_SEMI'
		},
	],
	[
		/^,/,
		function () {
			return 'TOK_COMMA'
		},
	],
	[
		/^[ \t]/,
		function () {
			/* skip whitespace */
		},
	],
	[
		/^"[^\"]+"/,
		function () {
			return 'STRING' /* " */
		},
	],
	[
		/^(\n|\r\n)/,
		function () {
			return 'EOL'
		},
	],
	[
		/^[a-zA-Z]+\.h/,
		function () {
			return 'IMPORT_ID'
		},
	],
	[
		/^\*?[a-zA-Z_]+/,
		function () {
			return 'IDENTIFIER'
		},
	],
	[
		/^[0-9]+/,
		function () {
			return 'INTEGER'
		},
	],
	[
		/^./,
		function () {
			/* */
		},
	],
]
const lexRulesByConditions = {
	INITIAL: [
		0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
		20, 21, 22, 23, 24, 25, 26, 27, 28,
	],
}

const EOF_TOKEN = {
	type: EOF,
	value: '',
}

tokenizer = {
	initString(string) {
		this._string = string
		this._cursor = 0

		this._states = ['INITIAL']
		this._tokensQueue = []

		this._currentLine = 1
		this._currentColumn = 0
		this._currentLineBeginOffset = 0

		/**
		 * Matched token location data.
		 */
		this._tokenStartOffset = 0
		this._tokenEndOffset = 0
		this._tokenStartLine = 1
		this._tokenEndLine = 1
		this._tokenStartColumn = 0
		this._tokenEndColumn = 0

		return this
	},

	/**
	 * Returns tokenizer states.
	 */
	getStates() {
		return this._states
	},

	getCurrentState() {
		return this._states[this._states.length - 1]
	},

	pushState(state) {
		this._states.push(state)
	},

	begin(state) {
		this.pushState(state)
	},

	popState() {
		if (this._states.length > 1) {
			return this._states.pop()
		}
		return this._states[0]
	},

	getNextToken() {
		// Something was queued, return it.
		if (this._tokensQueue.length > 0) {
			return this.onToken(this._toToken(this._tokensQueue.shift()))
		}

		if (!this.hasMoreTokens()) {
			return this.onToken(EOF_TOKEN)
		}

		let string = this._string.slice(this._cursor)
		let lexRulesForState = lexRulesByConditions[this.getCurrentState()]

		for (let i = 0; i < lexRulesForState.length; i++) {
			let lexRuleIndex = lexRulesForState[i]
			let lexRule = lexRules[lexRuleIndex]

			let matched = this._match(string, lexRule[0])

			// Manual handling of EOF token (the end of string). Return it
			// as `EOF` symbol.
			if (string === '' && matched === '') {
				this._cursor++
			}

			if (matched !== null) {
				yytext = matched
				yyleng = yytext.length
				let token = lexRule[1].call(this)

				if (!token) {
					return this.getNextToken()
				}

				// If multiple tokens are returned, save them to return
				// on next `getNextToken` call.

				if (Array.isArray(token)) {
					const tokensToQueue = token.slice(1)
					token = token[0]
					if (tokensToQueue.length > 0) {
						this._tokensQueue.unshift(...tokensToQueue)
					}
				}

				return this.onToken(this._toToken(token, yytext))
			}
		}

		if (this.isEOF()) {
			this._cursor++
			return EOF_TOKEN
		}

		this.throwUnexpectedToken(
			string[0],
			this._currentLine,
			this._currentColumn
		)
	},

	/**
	 * Throws default "Unexpected token" exception, showing the actual
	 * line from the source, pointing with the ^ marker to the bad token.
	 * In addition, shows `line:column` location.
	 */
	throwUnexpectedToken(symbol, line, column) {
		const lineSource = this._string.split('\n')[line - 1]
		let lineData = ''

		if (lineSource) {
			const pad = ' '.repeat(column)
			lineData = '\n\n' + lineSource + '\n' + pad + '^\n'
		}

		throw new SyntaxError(
			`${lineData}Unexpected token: "${symbol}" ` +
				`at ${line}:${column}.`
		)
	},

	getCursor() {
		return this._cursor
	},

	getCurrentLine() {
		return this._currentLine
	},

	getCurrentColumn() {
		return this._currentColumn
	},

	_captureLocation(matched) {
		const nlRe = /\n/g

		// Absolute offsets.
		this._tokenStartOffset = this._cursor

		// Line-based locations, start.
		this._tokenStartLine = this._currentLine
		this._tokenStartColumn =
			this._tokenStartOffset - this._currentLineBeginOffset

		// Extract `\n` in the matched token.
		let nlMatch
		while ((nlMatch = nlRe.exec(matched)) !== null) {
			this._currentLine++
			this._currentLineBeginOffset =
				this._tokenStartOffset + nlMatch.index + 1
		}

		this._tokenEndOffset = this._cursor + matched.length

		// Line-based locations, end.
		this._tokenEndLine = this._currentLine
		this._tokenEndColumn = this._currentColumn =
			this._tokenEndOffset - this._currentLineBeginOffset
	},

	_toToken(tokenType, yytext = '') {
		return {
			// Basic data.
			type: tokenType,
			value: yytext,

			// Location data.
			startOffset: this._tokenStartOffset,
			endOffset: this._tokenEndOffset,
			startLine: this._tokenStartLine,
			endLine: this._tokenEndLine,
			startColumn: this._tokenStartColumn,
			endColumn: this._tokenEndColumn,
		}
	},

	isEOF() {
		return this._cursor === this._string.length
	},

	hasMoreTokens() {
		return this._cursor <= this._string.length
	},

	_match(string, regexp) {
		let matched = string.match(regexp)
		if (matched) {
			// Handle `\n` in the matched token to track line numbers.
			this._captureLocation(matched[0])
			this._cursor += matched[0].length
			return matched[0]
		}
		return null
	},

	/**
	 * Allows analyzing, and transforming token. Default implementation
	 * just passes the token through.
	 */
	onToken(token) {
		return token
	},
}

/**
 * Expose tokenizer so it can be accessed in semantic actions.
 */
yy.lexer = tokenizer
yy.tokenizer = tokenizer

/**
 * Global parsing options. Some options can be shadowed per
 * each `parse` call, if the optations are passed.
 *
 * Initalized to the `captureLocations` which is passed
 * from the generator. Other options can be added at runtime.
 */
yy.options = {
	captureLocations: false,
}

/**
 * Parsing module.
 */
const yyparse = {
	/**
	 * Sets global parsing options.
	 */
	setOptions(options) {
		yy.options = options
		return this
	},

	/**
	 * Returns parsing options.
	 */
	getOptions() {
		return yy.options
	},

	/**
	 * Parses a string.
	 */
	parse(string, parseOptions) {
		if (!tokenizer) {
			throw new Error(`Tokenizer instance wasn't specified.`)
		}

		tokenizer.initString(string)

		/**
		 * If parse options are passed, override global parse options for
		 * this call, and later restore global options.
		 */
		let globalOptions = yy.options
		if (parseOptions) {
			yy.options = Object.assign({}, yy.options, parseOptions)
		}

		/**
		 * Allow callers to do setup work based on the
		 * parsing string, and passed options.
		 */
		yyparse.onParseBegin(string, tokenizer, yy.options)

		stack.length = 0
		stack.push(0)

		let token = tokenizer.getNextToken()
		let shiftedToken = null

		do {
			if (!token) {
				// Restore options.
				yy.options = globalOptions
				unexpectedEndOfInput()
			}

			let state = stack[stack.length - 1]
			let column = tokens[token.type]

			if (!table[state].hasOwnProperty(column)) {
				yy.options = globalOptions
				unexpectedToken(token)
			}

			let entry = table[state][column]

			// Shift action.
			if (entry[0] === 's') {
				let loc = null

				if (yy.options.captureLocations) {
					loc = {
						startOffset: token.startOffset,
						endOffset: token.endOffset,
						startLine: token.startLine,
						endLine: token.endLine,
						startColumn: token.startColumn,
						endColumn: token.endColumn,
					}
				}

				shiftedToken = this.onShift(token)

				stack.push(
					{
						symbol: tokens[shiftedToken.type],
						semanticValue: shiftedToken.value,
						loc,
					},
					Number(entry.slice(1))
				)

				token = tokenizer.getNextToken()
			}

			// Reduce action.
			else if (entry[0] === 'r') {
				let productionNumber = entry.slice(1)
				let production = productions[productionNumber]
				let hasSemanticAction = typeof production[2] === 'function'
				let semanticValueArgs = hasSemanticAction ? [] : null

				const locationArgs =
					hasSemanticAction && yy.options.captureLocations ? [] : null

				if (production[1] !== 0) {
					let rhsLength = production[1]
					while (rhsLength-- > 0) {
						stack.pop()
						let stackEntry = stack.pop()

						if (hasSemanticAction) {
							semanticValueArgs.unshift(stackEntry.semanticValue)

							if (locationArgs) {
								locationArgs.unshift(stackEntry.loc)
							}
						}
					}
				}

				const reduceStackEntry = { symbol: production[0] }

				if (hasSemanticAction) {
					yytext = shiftedToken ? shiftedToken.value : null
					yyleng = shiftedToken ? shiftedToken.value.length : null

					const semanticActionArgs =
						locationArgs !== null
							? semanticValueArgs.concat(locationArgs)
							: semanticValueArgs

					production[2](...semanticActionArgs)

					reduceStackEntry.semanticValue = __

					if (locationArgs) {
						reduceStackEntry.loc = __loc
					}
				}

				const nextState = stack[stack.length - 1]
				const symbolToReduceWith = production[0]

				stack.push(
					reduceStackEntry,
					table[nextState][symbolToReduceWith]
				)
			}

			// Accept.
			else if (entry === 'acc') {
				stack.pop()
				let parsed = stack.pop()

				if (
					stack.length !== 1 ||
					stack[0] !== 0 ||
					tokenizer.hasMoreTokens()
				) {
					// Restore options.
					yy.options = globalOptions
					unexpectedToken(token)
				}

				if (parsed.hasOwnProperty('semanticValue')) {
					yy.options = globalOptions
					yyparse.onParseEnd(parsed.semanticValue)
					return parsed.semanticValue
				}

				yyparse.onParseEnd()

				// Restore options.
				yy.options = globalOptions
				return true
			}
		} while (tokenizer.hasMoreTokens() || stack.length > 1)
	},

	setTokenizer(customTokenizer) {
		tokenizer = customTokenizer
		return yyparse
	},

	getTokenizer() {
		return tokenizer
	},

	onParseBegin(string, tokenizer, options) {},
	onParseEnd(parsed) {},

	/**
	 * Allows analyzing, and transforming shifted token. Default implementation
	 * just passes the token through.
	 */
	onShift(token) {
		return token
	},
}

function unexpectedToken(token) {
	if (token.type === EOF) {
		unexpectedEndOfInput()
	}

	tokenizer.throwUnexpectedToken(
		token.value,
		token.startLine,
		token.startColumn
	)
}

function unexpectedEndOfInput() {
	parseError(`Unexpected end of input.`)
}

function parseError(message) {
	throw new SyntaxError(message)
}

module.exports = yyparse
